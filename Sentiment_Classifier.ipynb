{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "06UnMqOJumRC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MXK4Bh9LvqBv"
      },
      "outputs": [],
      "source": [
        "# Artificial Dataset\n",
        "corpus = [\n",
        "    # Positive Reviews\n",
        "    'This is an excellent movie',\n",
        "    'The move was fantastic I like it',\n",
        "    'You should watch it is brilliant',\n",
        "    'Exceptionally good',\n",
        "    'Wonderfully directed and executed I like it',\n",
        "    'Its a fantastic series',\n",
        "    'Never watched such a brillent movie',\n",
        "    'It is a Wonderful movie',\n",
        "\n",
        "    # Negtive Reviews\n",
        "    \"horrible acting\",\n",
        "    'waste of money',\n",
        "    'pathetic picture',\n",
        "    'It was very boring',\n",
        "    'I did not like the movie',\n",
        "    'The movie was horrible',\n",
        "    'I will not recommend',\n",
        "    'The acting is pathetic'\n",
        "]\n",
        "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_file(file_name):\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        conversations = f\n",
        "    return conversations\n",
        "    \n",
        "corpus = load_file(\"data/train/formatted_single_dialogues_train.txt\")\n",
        "sentiments = load_file(\"data/train/formatted_single_dialogues_emotion_train.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSOXM4SG53dT",
        "outputId": "4de0b3d4-b835-495f-87d7-ba2b2f55715a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "vocab_length = 50\n",
        "\n",
        "all_words = []\n",
        "for sent in corpus:\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    for word in tokenize_word:\n",
        "        all_words.append(word)\n",
        "\n",
        "unique_words = set(all_words)\n",
        "\n",
        "embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n",
        "\n",
        "word_count = lambda sentence: len(word_tokenize(sentence))\n",
        "longest_sentence = max(corpus, key=word_count)\n",
        "length_long_sentence = len(word_tokenize(longest_sentence))\n",
        "\n",
        "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHhc_ho17obK",
        "outputId": "07159348-53fc-4fd8-b612-cb1805463431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 7, 20)             1000      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 140)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 141       \n",
            "=================================================================\n",
            "Total params: 1,141\n",
            "Trainable params: 1,141\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
        "#model.add(LSTM(64))\n",
        "\n",
        "#model.add(Conv1D(128, 5, activation='relu'))\n",
        "#model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL1nvELO76X6",
        "outputId": "580e5051-db9e-41f2-99f0-c4352906b5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 0.6931 - acc: 0.5000\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6898 - acc: 0.5625\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6865 - acc: 0.6875\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6833 - acc: 0.6875\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6800 - acc: 0.6875\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6767 - acc: 0.8750\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6735 - acc: 0.9375\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6702 - acc: 1.0000\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6670 - acc: 1.0000\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6637 - acc: 1.0000\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6605 - acc: 1.0000\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6572 - acc: 1.0000\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6539 - acc: 1.0000\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6507 - acc: 1.0000\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6474 - acc: 0.9375\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6441 - acc: 0.9375\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6408 - acc: 0.9375\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6375 - acc: 0.9375\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6341 - acc: 0.9375\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6307 - acc: 0.9375\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6273 - acc: 0.9375\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6239 - acc: 0.9375\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6205 - acc: 0.9375\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6170 - acc: 0.9375\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6135 - acc: 0.9375\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6100 - acc: 0.9375\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6064 - acc: 0.9375\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6028 - acc: 0.9375\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5992 - acc: 0.9375\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5955 - acc: 0.9375\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5918 - acc: 0.9375\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5881 - acc: 0.9375\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5843 - acc: 0.9375\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5805 - acc: 0.9375\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5766 - acc: 0.9375\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5728 - acc: 0.9375\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5689 - acc: 0.9375\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5649 - acc: 0.9375\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5609 - acc: 0.9375\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5569 - acc: 0.9375\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5529 - acc: 0.9375\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5488 - acc: 0.9375\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5447 - acc: 0.9375\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5405 - acc: 0.9375\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5363 - acc: 0.9375\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5321 - acc: 0.9375\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5279 - acc: 0.9375\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5236 - acc: 0.9375\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5193 - acc: 0.9375\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5150 - acc: 0.9375\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe502431210>"
            ]
          },
          "execution_count": 43,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(padded_sentences, sentiments, epochs=50, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBJhAGGA786F",
        "outputId": "29733b0e-b6ec-4cd0-982d-0949474cbf5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 93.750000\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVphmZXN8djN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Sentiment Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
